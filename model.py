# -*- coding: utf-8 -*-
"""behavioural_cloning.ipynb

Automatically generated by Colaboratory.

Original file is located at ****
    
"""
# Mount Google Drive
from google.colab import drive
drive.mount('/content/gdrive')


# Copy files from GDrive to local enviroment to improve speed

from shutil import copyfile
fileList = ['cardata1.zip','cardata1.csv', 'cardata2.zip','cardata2.csv','cardata3.zip', 'cardata3.csv','cardata4.zip','cardata4.csv','cardata_t2.zip', 'cardata_t2.csv','cardata_t2_2.zip', 'cardata_t2_2.csv']
for name in fileList:  
  src = '/content/gdrive/My Drive/ML_Data/Selfdriving/cardata/' + name
  dst = '/content/sample_data/' + name
  copyfile(src, dst)

#Some imports

import csv
import cv2
import numpy as np
import zipfile
import random
from tqdm import tqdm
import datetime
import matplotlib.pyplot as plt

#Testing how to read images from zipfile
import zipfile
archive = zipfile.ZipFile('/content/sample_data/cardata4.zip', 'r')

imgfile = archive.open('right_2018_11_16_11_04_58_748.jpg')
from PIL import Image
img = Image.open(imgfile)
plt.imshow(img)


# Correction on left and right side images
correction = 0.25
# Array to store all measurements
measurements = []
# Array to store all iamges
images = []
# Base output file name
outputFile = 'model_t1_t2'
# Checkpoint counters
fileCounter = 0

#Load image with name:name from zip archive
def load_image(archive, name):
  imgfile = archive.open(name.split("/")[-1])
  img = Image.open(imgfile)
  return np.array(img)

# Read all data for name.csv and name.zip into the images and measurements arrays. correction is
# used to modify steering values for left and right images
def readData(name, images, measurements, correction):
  lines = []

  with open('/content/sample_data/'+name+'.csv') as csvfile:
      reader = csv.reader(csvfile)
      for line in reader:
          lines.append(line)
  archive = zipfile.ZipFile('/content/sample_data/'+name+'.zip', 'r')
  

  
  for i in tqdm(range(len(lines))):
    line = lines[i]
    steering_center = float(line[3])
    steering_left = steering_center + correction
    steering_right = steering_center - correction
    image_center = load_image(archive, line[0])
    image_left = load_image(archive, line[1])
    image_right = load_image(archive, line[2])
    images.extend([image_center, image_left, image_right])
    measurements.extend([steering_center, steering_left, steering_right])
    #images.extend([np.fliplr(image_center),np.fliplr(image_left), np.fliplr(image_right)])
    #measurements.extend([steering_center * -1.0, steering_left* -1.0, steering_right* -1.0])


# Datasets to read
readData('cardata_t2',images, measurements,correction)
readData('cardata4',images, measurements,correction)
readData('cardata_t2_2',images, measurements,correction)


from keras.models import Sequential
from keras.layers import Flatten, Dense, Lambda, Conv2D, MaxPooling2D, Cropping2D, Dropout, Activation
from keras.layers.normalization import BatchNormalization

#Building the model

model = Sequential()
model.add(Lambda(lambda x: (x / 127.5) - 1, input_shape=(160,320,3)))
model.add(Cropping2D(cropping=((75,15), (0,0))))

model.add(Conv2D(24,5,strides=(2, 2)))
model.add(BatchNormalization())
model.add(Activation('relu'))

model.add(Conv2D(36,5,strides=(2, 2)))
model.add(BatchNormalization())
model.add(Activation('relu'))

model.add(Conv2D(48,5,strides=(2, 2)))
model.add(BatchNormalization())
model.add(Activation('relu'))


model.add(Conv2D(64,3))
model.add(BatchNormalization())
model.add(Activation('relu'))


model.add(Conv2D(64,3))
model.add(BatchNormalization())
model.add(Activation('relu'))


model.add(MaxPooling2D())

model.add(Flatten())
model.add(Dense(128))
model.add(Dropout(0.5))
model.add(Dense(64))
model.add(Dropout(0.5))
model.add(Dense(8))
model.add(Dropout(0.5))
model.add(Dense(1))

model.compile(loss='mse', optimizer='adam')

# Transformation functions
# most of the image functions come from:
# https://medium.freecodecamp.org/image-augmentation-make-it-rain-make-it-snow-how-to-modify-a-photo-with-machine-learning-163c0cb3843f

# Randomly add or remove brighness
def add_brightness(image):
    image_HLS = cv2.cvtColor(image,cv2.COLOR_RGB2HLS) ## Conversion to HLS
    image_HLS = np.array(image_HLS, dtype = np.float64) 
    random_brightness_coefficient = np.random.uniform()*1.6+0.2 ## generates value between 0.5 and 1.5
    image_HLS[:,:,1] = image_HLS[:,:,1]*random_brightness_coefficient ## scale pixel values up or down for channel 1(Lightness)
    image_HLS[:,:,1][image_HLS[:,:,1]>255]  = 255 ##Sets all values above 255 to 255
    image_HLS = np.array(image_HLS, dtype = np.uint8)
    image_RGB = cv2.cvtColor(image_HLS,cv2.COLOR_HLS2RGB) ## Conversion to RGB
    return image_RGB

def generate_shadow_coordinates(imshape, no_of_shadows=1):
    vertices_list=[]
    for index in range(no_of_shadows):
        vertex=[]
        for dimensions in range(np.random.randint(3,15)): ## Dimensionality of the shadow polygon
            vertex.append(( imshape[1]*np.random.uniform(),imshape[0]//3+imshape[0]*np.random.uniform()))
        vertices = np.array([vertex], dtype=np.int32) ## single shadow vertices 
        vertices_list.append(vertices)
    return vertices_list ## List of shadow vertices

# Randomly add no_of_shadows shadow polygons
def add_shadow(image, no_of_shadows=1):
    image_HLS = cv2.cvtColor(image,cv2.COLOR_RGB2HLS) ## Conversion to HLS
    mask = np.zeros_like(image) 
    imshape = image.shape
    vertices_list= generate_shadow_coordinates(imshape, no_of_shadows) #3 getting list of shadow vertices
    for vertices in vertices_list: 
        cv2.fillPoly(mask, vertices, 255) ## adding all shadow polygons on empty mask, single 255 denotes only red channel
    
    image_HLS[:,:,1][mask[:,:,0]==255] = image_HLS[:,:,1][mask[:,:,0]==255]*0.5   ## if red channel is hot, image's "Lightness" channel's brightness is lowered 
    image_RGB = cv2.cvtColor(image_HLS,cv2.COLOR_HLS2RGB) ## Conversion to RGB
    return image_RGB  
  
# Randomly mirror the image
def mirror(image, angle):
  if random.random() > 0.5:
     return (np.fliplr(image), angle * -1)
  return image, angle

#mirror test
m_img, steer = mirror(img,1)
print(steer)
plt.imshow(m_img)

plt.imshow(add_shadow(np.array(img),no_of_shadows=1))

plt.imshow(add_brightness(np.array(img)))

# Some Imports
from sklearn.model_selection import train_test_split
import cv2
import numpy as np
import sklearn
import itertools



# build generator for from samples and mesaures and a certain batch size

def generator(samples, measures, batch_size=32):
    num_samples = len(samples)
    #augmented_samples = list(itertools.product(samples, [aug for aug in Augmentation]))
    while 1: # Loop forever so the generator never terminates
        _samples,_measures = sklearn.utils.shuffle(samples, measures)

        for offset in range(0, num_samples, batch_size):
            images = _samples[offset:offset+batch_size]
            angles = _measures[offset:offset+batch_size]
            for i in range(len(images)):
              image = images[i]
              angle = angles[i]
              image,angle = mirror(image, angle)
              image = add_brightness(image)
              image = add_shadow(image, random.randint(0,1))
              
              images[i] = image
              angles[i] = angle
                  
            # trim image to only see section with road
            X_train = np.array(images)
            y_train = np.array(angles)
            yield X_train, y_train

import matplotlib.pyplot as plt

# Draw a plot of training history
def plot_history(history_object):
    ### print the keys contained in the history object
    print(history_object.history.keys())

    ### plot the training and validation loss for each epoch
    plt.plot(history_object.history['loss'])
    plt.plot(history_object.history['val_loss'])
    plt.title('model mean squared error loss')
    plt.ylabel('mean squared error loss')
    plt.xlabel('epoch')
    plt.legend(['training set', 'validation set'], loc='upper right')
    plt.show()

# Shuffle data
sklearn.utils.shuffle(images, measurements)

# Generate training and validation data
train_samples, validation_samples, train_measures, validation_measures = train_test_split(images, measurements, test_size=0.2)

batch_size = 256

# Create generators
train_generator = generator(train_samples, train_measures, batch_size=batch_size)
validation_generator = generator(validation_samples, validation_measures, batch_size=batch_size)

# Fit with 20 epochs
print('Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now()))
history_object = model.fit_generator(
    train_generator,
    steps_per_epoch=len(train_samples)/batch_size, 
    validation_data=validation_generator,
    validation_steps = len(validation_samples)/batch_size, 
    epochs=20, 
    verbose=1
)


# Plot and save checkpoint
plot_history(history_object)
fileCounter += 1
model.save('/content/gdrive/My Drive/ML_Data/Selfdriving/cardata/'+outputFile+str(fileCounter)+'.h5')


# Create generators and fit with 20 epochs
train_generator = generator(train_samples, train_measures, batch_size=batch_size)
validation_generator = generator(validation_samples, validation_measures, batch_size=batch_size)
print('Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now()))
history_object = model.fit_generator(
    train_generator,
    steps_per_epoch=len(train_samples)/batch_size, 
    validation_data=validation_generator,
    validation_steps = len(validation_samples)/batch_size, 
    epochs=20, 
    verbose=1
)

# Plot and save checkpoint
plot_history(history_object)
fileCounter += 1
model.save('/content/gdrive/My Drive/ML_Data/Selfdriving/cardata/'+outputFile+str(fileCounter)+'.h5')

# Create generators and fit with 20 epochs
train_generator = generator(train_samples, train_measures, batch_size=batch_size)
validation_generator = generator(validation_samples, validation_measures, batch_size=batch_size)
print('Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now()))
history_object = model.fit_generator(
    train_generator,
    steps_per_epoch=len(train_samples)/batch_size, 
    validation_data=validation_generator,
    validation_steps = len(validation_samples)/batch_size, 
    epochs=20, 
    verbose=1
)

# Plot and save checkpoint
plot_history(history_object)
fileCounter += 1
model.save('/content/gdrive/My Drive/ML_Data/Selfdriving/cardata/'+outputFile+str(fileCounter)+'.h5')

# Create generators and fit with 20 epochs
train_generator = generator(train_samples, train_measures, batch_size=batch_size)
validation_generator = generator(validation_samples, validation_measures, batch_size=batch_size)
print('Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now()))
history_object = model.fit_generator(
    train_generator,
    steps_per_epoch=len(train_samples)/batch_size, 
    validation_data=validation_generator,
    validation_steps = len(validation_samples)/batch_size, 
    epochs=20, 
    verbose=1
)

# Plot and save checkpoint
plot_history(history_object)
fileCounter += 1
model.save('/content/gdrive/My Drive/ML_Data/Selfdriving/cardata/'+outputFile+str(fileCounter)+'.h5')

# Create generators and fit with 20 epochs
train_generator = generator(train_samples, train_measures, batch_size=batch_size)
validation_generator = generator(validation_samples, validation_measures, batch_size=batch_size)
print('Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now()))
history_object = model.fit_generator(
    train_generator,
    steps_per_epoch=len(train_samples)/batch_size, 
    validation_data=validation_generator,
    validation_steps = len(validation_samples)/batch_size, 
    epochs=20, 
    verbose=1
)

# Plot and save checkpoint
plot_history(history_object)
fileCounter += 1
model.save('/content/gdrive/My Drive/ML_Data/Selfdriving/cardata/'+outputFile+str(fileCounter)+'.h5')

# Create generators and fit with 20 epochs
train_generator = generator(train_samples, train_measures, batch_size=batch_size)
validation_generator = generator(validation_samples, validation_measures, batch_size=batch_size)
print('Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now()))
history_object = model.fit_generator(
    train_generator,
    steps_per_epoch=len(train_samples)/batch_size, 
    validation_data=validation_generator,
    validation_steps = len(validation_samples)/batch_size, 
    epochs=20, 
    verbose=1
)

# Plot and save checkpoint
plot_history(history_object)
fileCounter += 1
model.save('/content/gdrive/My Drive/ML_Data/Selfdriving/cardata/'+outputFile+str(fileCounter)+'.h5')

# Create generators and fit with 20 epochs
train_generator = generator(train_samples, train_measures, batch_size=batch_size)
validation_generator = generator(validation_samples, validation_measures, batch_size=batch_size)
print('Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now()))
history_object = model.fit_generator(
    train_generator,
    steps_per_epoch=len(train_samples)/batch_size, 
    validation_data=validation_generator,
    validation_steps = len(validation_samples)/batch_size, 
    epochs=20, 
    verbose=1
)

# Plot and save checkpoint
plot_history(history_object)
fileCounter += 1
model.save('/content/gdrive/My Drive/ML_Data/Selfdriving/cardata/'+outputFile+str(fileCounter)+'.h5')

# Create generators and fit with 20 epochs
train_generator = generator(train_samples, train_measures, batch_size=batch_size)
validation_generator = generator(validation_samples, validation_measures, batch_size=batch_size)
print('Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now()))
history_object = model.fit_generator(
    train_generator,
    steps_per_epoch=len(train_samples)/batch_size, 
    validation_data=validation_generator,
    validation_steps = len(validation_samples)/batch_size, 
    epochs=20, 
    verbose=1
)

# Plot and save checkpoint
plot_history(history_object)
fileCounter += 1
model.save('/content/gdrive/My Drive/ML_Data/Selfdriving/cardata/'+outputFile+str(fileCounter)+'.h5')

# Create generators and fit with 20 epochs
train_generator = generator(train_samples, train_measures, batch_size=batch_size)
validation_generator = generator(validation_samples, validation_measures, batch_size=batch_size)
print('Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now()))
history_object = model.fit_generator(
    train_generator,
    steps_per_epoch=len(train_samples)/batch_size, 
    validation_data=validation_generator,
    validation_steps = len(validation_samples)/batch_size, 
    epochs=20, 
    verbose=1
)

# Plot and save checkpoint
plot_history(history_object)
fileCounter += 1
model.save('/content/gdrive/My Drive/ML_Data/Selfdriving/cardata/'+outputFile+str(fileCounter)+'.h5')

# Create generators and fit with 20 epochs
train_generator = generator(train_samples, train_measures, batch_size=batch_size)
validation_generator = generator(validation_samples, validation_measures, batch_size=batch_size)
print('Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now()))
history_object = model.fit_generator(
    train_generator,
    steps_per_epoch=len(train_samples)/batch_size, 
    validation_data=validation_generator,
    validation_steps = len(validation_samples)/batch_size, 
    epochs=20, 
    verbose=1
)

# Plot and save checkpoint
plot_history(history_object)
fileCounter += 1
model.save('/content/gdrive/My Drive/ML_Data/Selfdriving/cardata/'+outputFile+str(fileCounter)+'.h5')

